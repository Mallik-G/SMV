#!/usr/bin/env bash

set -e

SMV_TOOLS="$(cd "`dirname "$0"`"; pwd)"
source $SMV_TOOLS/_env.sh

export SPARK_PRINT_LAUNCH_COMMAND=1

# Another way to add smv.py is through the --py-files option passed to
# pyspark, as in `pyspark --py-files $SMV_TOOLS/../python/smv.py`
# Not sure yet which way is best practice.
export PYTHONPATH="$PYTHONPATH:$SMV_TOOLS/../python"

# PySpark pre-2.0.0 has a bug (see
# https://issues.apache.org/jira/browse/SPARK-5185) that does not add
# the jar file to the driver's classpath, so we need to add the jars
# to the --driver-class-path command-line option
pyspark --executor-memory ${EXECUTOR_MEMORY} --driver-memory ${DRIVER_MEMORY} --master ${MASTER} --jars "$APP_JAR" --driver-class-path "${APP_JAR}" ${ARGS[@]}
